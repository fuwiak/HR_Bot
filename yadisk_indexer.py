"""
Yandex Disk Indexer - —Ñ–æ–Ω–æ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –≤ Qdrant Cloud
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞–Ω–∏—Ä—É–µ—Ç –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
"""
import os
import sys
import logging
import asyncio
import tempfile
from pathlib import Path
from typing import List, Dict, Optional, Set
from datetime import datetime
import hashlib

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('yadisk_indexer.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
log = logging.getLogger(__name__)

# ===================== CONFIGURATION =====================

# –ü–∞–ø–∫–∏ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–æ—Ä–µ–Ω—å)
WATCH_FOLDERS = os.getenv("YADISK_WATCH_FOLDERS", "/").split(",")

# –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
SCAN_INTERVAL = int(os.getenv("YADISK_SCAN_INTERVAL", "300"))  # 5 –º–∏–Ω—É—Ç

# –ö–æ–ª–ª–µ–∫—Ü–∏—è Qdrant
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "hr_knowledge_base")

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–≤ –ú–ë)
MAX_FILE_SIZE_MB = int(os.getenv("YADISK_MAX_FILE_SIZE", "50"))

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
SUPPORTED_EXTENSIONS = {
    '.txt', '.md', '.pdf', 
    '.doc', '.docx', 
    '.xls', '.xlsx',
    '.csv', '.json', '.xml'
}

# ===================== IMPORTS =====================

try:
    from yandex_disk_helper import (
        list_files, 
        download_file_content,
        get_file_type
    )
    from qdrant_helper import (
        get_qdrant_client,
        generate_embedding_async
    )
    from text_splitter import RecursiveCharacterTextSplitter
    log.info("‚úÖ –í—Å–µ –º–æ–¥—É–ª–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")
except ImportError as e:
    log.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    sys.exit(1)

# ===================== FILE PROCESSING =====================

def get_file_hash(content: bytes) -> str:
    """–ü–æ–ª—É—á–∏—Ç—å —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
    return hashlib.md5(content).hexdigest()

def extract_text_from_content(content: bytes, filename: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
    
    Args:
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö
        filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞
    
    Returns:
        –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ None
    """
    ext = Path(filename).suffix.lower()
    
    try:
        # TXT, MD, JSON, XML, CSV
        if ext in ['.txt', '.md', '.json', '.xml', '.csv']:
            for encoding in ['utf-8', 'cp1251', 'latin-1']:
                try:
                    return content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å {filename}")
            return None
        
        # PDF
        elif ext == '.pdf':
            try:
                import PyPDF2
                import io
                
                pdf_file = io.BytesIO(content)
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                text = ""
                
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                
                return text.strip()
            except Exception as e:
                log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF {filename}: {e}")
                return None
        
        # DOCX
        elif ext == '.docx':
            try:
                from docx import Document
                import io
                
                doc_file = io.BytesIO(content)
                doc = Document(doc_file)
                text = "\n".join([para.text for para in doc.paragraphs])
                
                return text.strip()
            except Exception as e:
                log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ DOCX {filename}: {e}")
                return None
        
        # DOC (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
        elif ext == '.doc':
            log.warning(f"‚ö†Ô∏è –§–æ—Ä–º–∞—Ç .doc –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é: {filename}")
            return None
        
        # XLSX
        elif ext in ['.xlsx', '.xls']:
            try:
                import pandas as pd
                import io
                
                excel_file = io.BytesIO(content)
                
                # –ß–∏—Ç–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã
                dfs = pd.read_excel(excel_file, sheet_name=None, engine='openpyxl' if ext == '.xlsx' else 'xlrd')
                
                text = ""
                for sheet_name, df in dfs.items():
                    text += f"\n=== {sheet_name} ===\n"
                    text += df.to_string(index=False)
                    text += "\n"
                
                return text.strip()
            except Exception as e:
                log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Excel {filename}: {e}")
                return None
        
        else:
            log.warning(f"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {filename}")
            return None
            
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {filename}: {e}")
        return None

# ===================== QDRANT OPERATIONS =====================

async def index_document(
    file_path: str,
    file_name: str,
    content: bytes,
    file_hash: str,
    modified: str
) -> bool:
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ Qdrant
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ
        file_name: –ò–º—è —Ñ–∞–π–ª–∞
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        file_hash: MD5 —Ö–µ—à —Ñ–∞–π–ª–∞
        modified: –î–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    
    Returns:
        True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
    """
    try:
        log.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_name}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = extract_text_from_content(content, file_name)
        
        if not text or len(text.strip()) < 50:
            log.warning(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –≤ {file_name}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return False
        
        log.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {file_name}")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        chunks = text_splitter.split_text(text)
        log.info(f"üì¶ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {file_name}")
        
        if not chunks:
            log.warning(f"‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è {file_name}")
            return False
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∏–µ–Ω—Ç Qdrant
        client = get_qdrant_client()
        
        if not client:
            log.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Qdrant")
            return False
        
        # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        from qdrant_client.models import PointStruct
        points = []
        
        for i, chunk in enumerate(chunks):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = await generate_embedding_async(chunk)
            
            if not embedding:
                log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞ {i} –∏–∑ {file_name}")
                continue
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            point_id = abs(hash(f"{file_hash}_{i}")) % (10 ** 10)
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "text": chunk,
                "source": "yadisk",
                "file_path": file_path,
                "file_name": file_name,
                "file_hash": file_hash,
                "chunk_index": i,
                "total_chunks": len(chunks),
                "modified": modified,
                "indexed_at": datetime.now().isoformat()
            }
            
            point = PointStruct(
                id=point_id,
                vector=embedding,
                payload=metadata
            )
            
            points.append(point)
        
        if not points:
            log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–æ—á–∫–∏ –¥–ª—è {file_name}")
            return False
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Qdrant –±–∞—Ç—á–∞–º–∏
        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i:i + batch_size]
            
            try:
                client.upsert(
                    collection_name=QDRANT_COLLECTION,
                    points=batch
                )
                log.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(batch)} —Ç–æ—á–µ–∫ ({i+1}-{i+len(batch)} –∏–∑ {len(points)}) –¥–ª—è {file_name}")
            except Exception as e:
                log.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞ –¥–ª—è {file_name}: {e}")
                return False
        
        log.info(f"üéâ –§–∞–π–ª {file_name} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω ({len(points)} —Ç–æ—á–µ–∫)")
        return True
        
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {file_name}: {e}")
        import traceback
        log.error(f"‚ùå Traceback: {traceback.format_exc()}")
        return False

# ===================== FILE SCANNING =====================

async def scan_folder(folder_path: str, processed_hashes: Set[str]) -> List[Dict]:
    """
    –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–∞–ø–∫—É –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ
    
    Args:
        folder_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ
        processed_hashes: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ö–µ—à–µ–π —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    try:
        log.info(f"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏: {folder_path}")
        
        result = await list_files(path=folder_path, limit=1000)
        
        if not result:
            log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ {folder_path}")
            return []
        
        items = result.get("_embedded", {}).get("items", [])
        files_to_process = []
        
        for item in items:
            item_type = item.get("type")
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–¥–ø–∞–ø–∫–∏
            if item_type == "dir":
                subfolder_path = item.get("path", "")
                log.info(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ –ø–æ–¥–ø–∞–ø–∫–∞: {subfolder_path}")
                # –ú–æ–∂–Ω–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞
                # subfiles = await scan_folder(subfolder_path, processed_hashes)
                # files_to_process.extend(subfiles)
                continue
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã
            if item_type != "file":
                continue
            
            name = item.get("name", "")
            path = item.get("path", "")
            size = item.get("size", 0)
            modified = item.get("modified", "")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            ext = Path(name).suffix.lower()
            if ext not in SUPPORTED_EXTENSIONS:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
            size_mb = size / (1024 * 1024)
            if size_mb > MAX_FILE_SIZE_MB:
                log.warning(f"‚ö†Ô∏è –§–∞–π–ª {name} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({size_mb:.1f} –ú–ë), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
            
            files_to_process.append({
                "name": name,
                "path": path,
                "size": size,
                "modified": modified
            })
        
        log.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(files_to_process)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ {folder_path}")
        return files_to_process
        
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è {folder_path}: {e}")
        return []

async def process_files(files: List[Dict], processed_hashes: Set[str]) -> int:
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
    
    Args:
        files: –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
        processed_hashes: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ö–µ—à–µ–π
    
    Returns:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    """
    success_count = 0
    
    for file_info in files:
        name = file_info["name"]
        path = file_info["path"]
        modified = file_info["modified"]
        
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
            log.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {name}")
            content = await download_file_content(path)
            
            if not content:
                log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {name}")
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Ö–µ—à
            file_hash = get_file_hash(content)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –ª–∏ —É–∂–µ
            if file_hash in processed_hashes:
                log.info(f"‚è≠Ô∏è –§–∞–π–ª {name} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
            success = await index_document(
                file_path=path,
                file_name=name,
                content=content,
                file_hash=file_hash,
                modified=modified
            )
            
            if success:
                processed_hashes.add(file_hash)
                success_count += 1
                log.info(f"‚úÖ [{success_count}] –£—Å–ø–µ—à–Ω–æ: {name}")
            else:
                log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å {name}")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏
            await asyncio.sleep(1)
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {name}: {e}")
            continue
    
    return success_count

# ===================== MAIN LOOP =====================

async def indexer_loop():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    log.info("üöÄ –ó–∞–ø—É—Å–∫ Yandex Disk Indexer")
    log.info(f"üìÇ –ü–∞–ø–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {WATCH_FOLDERS}")
    log.info(f"‚è±Ô∏è –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {SCAN_INTERVAL} —Å–µ–∫—É–Ω–¥")
    log.info(f"üì¶ –ö–æ–ª–ª–µ–∫—Ü–∏—è Qdrant: {QDRANT_COLLECTION}")
    log.info(f"üìè –ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {MAX_FILE_SIZE_MB} –ú–ë")
    log.info(f"üìÑ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {', '.join(SUPPORTED_EXTENSIONS)}")
    
    # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–ø–æ —Ö–µ—à—É)
    processed_hashes: Set[str] = set()
    
    iteration = 0
    
    while True:
        iteration += 1
        log.info(f"\n{'='*60}")
        log.info(f"üîÑ –ò–¢–ï–†–ê–¶–ò–Ø #{iteration} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        log.info(f"{'='*60}")
        
        try:
            all_files = []
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –ø–∞–ø–∫–∏
            for folder in WATCH_FOLDERS:
                folder = folder.strip()
                log.info(f"\nüìÇ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {folder}")
                files = await scan_folder(folder, processed_hashes)
                all_files.extend(files)
            
            log.info(f"\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_files)}")
            log.info(f"üìä –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_hashes)}")
            
            if all_files:
                log.info(f"\nüîß –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(all_files)} —Ñ–∞–π–ª–æ–≤...")
                success_count = await process_files(all_files, processed_hashes)
                
                log.info(f"\n‚úÖ –ò—Ç–µ—Ä–∞—Ü–∏—è #{iteration} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                log.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {success_count}")
                log.info(f"üìä –í—Å–µ–≥–æ –≤ –∫–µ—à–µ: {len(processed_hashes)} —Ñ–∞–π–ª–æ–≤")
            else:
                log.info(f"\nüí§ –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            
            # –ñ–¥–µ–º —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
            log.info(f"\n‚è≥ –°–ª–µ–¥—É—é—â–µ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ {SCAN_INTERVAL} —Å–µ–∫—É–Ω–¥...")
            await asyncio.sleep(SCAN_INTERVAL)
            
        except KeyboardInterrupt:
            log.info("\nüõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
            break
        except Exception as e:
            log.error(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
            import traceback
            log.error(f"‚ùå Traceback: {traceback.format_exc()}")
            log.info(f"‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {SCAN_INTERVAL} —Å–µ–∫—É–Ω–¥...")
            await asyncio.sleep(SCAN_INTERVAL)
    
    log.info("üëã Yandex Disk Indexer –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# ===================== ENTRY POINT =====================

def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    try:
        asyncio.run(indexer_loop())
    except KeyboardInterrupt:
        log.info("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        sys.exit(0)

if __name__ == "__main__":
    main()
