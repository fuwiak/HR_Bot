"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π LLM-–∫–ª–∏–µ–Ω—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π OpenRouter (–æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä).
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±—ã—Å—Ç—Ä—É—é –∑–∞–º–µ–Ω—É endpoint —á–µ—Ä–µ–∑ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ.
"""

import os
import httpx
import time
import asyncio
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    """–û—Ç–≤–µ—Ç –æ—Ç LLM API"""
    content: str
    provider: str
    model: str
    confidence: float = 1.0
    tokens_used: Optional[int] = None
    error: Optional[str] = None


class LLMClient:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM API"""
    
    def __init__(
        self,
        primary_provider: str = "openrouter",
        primary_model: str = "deepseek/deepseek-chat",
        fallback_chain: Optional[List[Dict[str, str]]] = None,
        confidence_threshold: float = 0.7,
        timeout: int = 30
    ):
        self.primary_provider = primary_provider
        self.primary_model = primary_model
        self.confidence_threshold = confidence_threshold
        self.timeout = timeout
        
        # –¶–µ–ø–æ—á–∫–∞ fallback –º–æ–¥–µ–ª–µ–π (—Ç–æ–ª—å–∫–æ OpenRouter)
        if fallback_chain is None:
            # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ fallback (—Ç–æ–ª—å–∫–æ OpenRouter –º–æ–¥–µ–ª–∏)
            self.fallback_chain = [
                {"provider": "openrouter", "model": "deepseek/deepseek-chat"},
                {"provider": "openrouter", "model": "meta-llama/llama-3.3-70b-instruct"}
            ]
        else:
            self.fallback_chain = fallback_chain
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ ENV (—Ç–æ–ª—å–∫–æ OpenRouter)
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        self.openrouter_api_key = openrouter_key.strip() if openrouter_key else None
        
        openrouter_url = os.getenv("OPENROUTER_API_URL", "https://openrouter.ai/api/v1/chat/completions")
        self.openrouter_api_url = openrouter_url.strip()
        
        # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ qroq/ollama –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ ENV
        # OpenRouter –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        
        # HTTP –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–µ—Ç—Å—è –ª–µ–Ω–∏–≤–æ –¥–ª—è thread-safety
        self._client: Optional[httpx.AsyncClient] = None
        self._client_lock = None
    
    def _get_client(self) -> httpx.AsyncClient:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç (thread-safe)"""
        if self._client_lock is None:
            import threading
            self._client_lock = threading.Lock()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç
        if self._client is None:
            with self._client_lock:
                # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                if self._client is None:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç
                    self._client = httpx.AsyncClient(timeout=self.timeout)
                    logger.debug("–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π HTTP –∫–ª–∏–µ–Ω—Ç")
        elif self._client.is_closed:
            # –ö–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç - –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º
            with self._client_lock:
                # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                if self._client.is_closed:
                    self._client = httpx.AsyncClient(timeout=self.timeout)
                    logger.debug("HTTP –∫–ª–∏–µ–Ω—Ç –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω (–±—ã–ª –∑–∞–∫—Ä—ã—Ç)")
        
        return self._client
    
    async def _ensure_client(self) -> httpx.AsyncClient:
        """–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ä–∞–±–æ—á–µ–≥–æ HTTP –∫–ª–∏–µ–Ω—Ç–∞ (async-safe)"""
        client = self._get_client()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–∫—Ä—ã—Ç –ª–∏ –∫–ª–∏–µ–Ω—Ç
        if client.is_closed:
            logger.warning("HTTP –∫–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é...")
            if self._client_lock is not None:
                with self._client_lock:
                    if self._client is not None and self._client.is_closed:
                        try:
                            await self._client.aclose()
                        except:
                            pass
                        self._client = None
            client = self._get_client()
        
        return client
    
    async def _call_api(
        self,
        provider: str,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2048
    ) -> LLMResponse:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ LLM API"""
        
        if provider == "openrouter":
            api_key = self.openrouter_api_key
            api_url = self.openrouter_api_url
            # –û—á–∏—â–∞–µ–º API –∫–ª—é—á –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤/–ø–µ—Ä–µ–Ω–æ—Å–æ–≤
            api_key = api_key.strip() if api_key else None
            api_url = api_url.strip()
            app_url = os.getenv("APP_URL", "https://hr2137-bot.railway.app").strip()
            headers = {
                "Authorization": f"Bearer {api_key}",
                "HTTP-Referer": app_url,
                "X-Title": "HR2137 Bot RAG",
                "Content-Type": "application/json"
            }
        else:
            raise ValueError(f"Unknown provider: {provider}")
        
        if not api_key:
            raise ValueError(f"{provider.upper()}_API_KEY not set in environment")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è URL (–ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç –Ω–µ–ø–µ—á–∞—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤)
        if '\n' in api_url or '\r' in api_url:
            logger.error(f"Invalid URL contains newline characters: {repr(api_url)}")
            api_url = api_url.replace('\n', '').replace('\r', '').strip()
            logger.warning(f"Cleaned URL: {api_url}")
        
        # –û—á–∏—â–∞–µ–º messages –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        cleaned_messages = []
        for msg in messages:
            cleaned_msg = {
                "role": msg.get("role", "user").strip(),
                "content": msg.get("content", "").strip()
            }
            # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            if cleaned_msg["content"]:
                cleaned_messages.append(cleaned_msg)
        
        if not cleaned_messages:
            raise ValueError("No valid messages to send")
        
        payload = {
            "model": model.strip() if isinstance(model, str) else model,
            "messages": cleaned_messages,
            "temperature": float(temperature),
            "max_tokens": int(max_tokens)
        }
        
        try:
            logger.info(f"üîµ [LLM API] –í—ã–∑–æ–≤ {provider} API —Å –º–æ–¥–µ–ª—å—é {model}")
            logger.info(f"üîµ [LLM API] URL: {api_url}")
            logger.info(f"üîµ [LLM API] –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: temperature={temperature}, max_tokens={max_tokens}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç—ã (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏)
            for idx, msg in enumerate(cleaned_messages):
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                preview = content[:500] + "..." if len(content) > 500 else content
                logger.info(f"üîµ [LLM API] –°–æ–æ–±—â–µ–Ω–∏–µ {idx+1} ({role}): {preview}")
            
            start_time = time.time()
            
            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–∞–±–æ—á–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
            client = await self._ensure_client()
            
            try:
                response = await client.post(
                    api_url,
                    headers=headers,
                    json=payload
                )
            except (RuntimeError, httpx.TransportError, httpx.RequestError) as e:
                # –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ - –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º
                error_str = str(e).lower()
                if "closed" in error_str or "client has been closed" in error_str:
                    logger.warning(f"HTTP –∫–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç ({str(e)}), –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é –∏ –ø–æ–≤—Ç–æ—Ä—è—é –∑–∞–ø—Ä–æ—Å...")
                    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–π –∫–ª–∏–µ–Ω—Ç
                    if self._client_lock is not None:
                        with self._client_lock:
                            if self._client is not None:
                                try:
                                    await self._client.aclose()
                                except:
                                    pass
                            self._client = None
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å
                    client = await self._ensure_client()
                    response = await client.post(
                        api_url,
                        headers=headers,
                        json=payload
                    )
                else:
                    raise
            
            elapsed_time = time.time() - start_time
            logger.info(f"‚úÖ [LLM API] –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed_time:.2f}s")
            
            response.raise_for_status()
            data = response.json()
            
            content = data["choices"][0]["message"]["content"]
            tokens_used = data.get("usage", {}).get("total_tokens")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç LLM (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤)
            content_preview = content[:500] + "..." if len(content) > 500 else content
            logger.info(f"‚úÖ [LLM API] –û—Ç–≤–µ—Ç –æ—Ç {provider}/{model}: {content_preview}")
            logger.info(f"‚úÖ [LLM API] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {tokens_used}")
            
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
            confidence = 1.0 if len(content) > 50 else 0.5
            logger.info(f"‚úÖ [LLM API] –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
            
            return LLMResponse(
                content=content,
                provider=provider,
                model=model,
                confidence=confidence,
                tokens_used=tokens_used
            )
            
        except httpx.TimeoutException:
            logger.error(f"‚ùå [LLM API] –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≤—ã–∑–æ–≤–µ {provider} API (–º–æ–¥–µ–ª—å: {model})")
            return LLMResponse(
                content="",
                provider=provider,
                model=model,
                confidence=0.0,
                error="Timeout"
            )
        except httpx.HTTPStatusError as e:
            error_detail = ""
            try:
                error_data = e.response.json()
                error_detail = f": {error_data}"
            except:
                try:
                    error_detail = f": {e.response.text[:200]}"
                except:
                    pass
            
            logger.error(f"‚ùå [LLM API] –û—à–∏–±–∫–∞ {provider} API: HTTP {e.response.status_code}{error_detail}")
            return LLMResponse(
                content="",
                provider=provider,
                model=model,
                confidence=0.0,
                error=f"HTTP {e.response.status_code}{error_detail}"
            )
        except Exception as e:
            logger.error(f"‚ùå [LLM API] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ {provider} API: {str(e)}")
            import traceback
            logger.error(f"‚ùå [LLM API] Traceback: {traceback.format_exc()}")
            return LLMResponse(
                content="",
                provider=provider,
                model=model,
                confidence=0.0,
                error=str(e)
            )
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        use_fallback: bool = True
    ) -> LLMResponse:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ fallback.
        
        Args:
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            model: –ú–æ–¥–µ–ª—å (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω–∞—è)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            use_fallback: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback –ø—Ä–∏ –æ—à–∏–±–∫–µ/–Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        
        Returns:
            LLMResponse —Å –æ—Ç–≤–µ—Ç–æ–º –∏–ª–∏ –æ—à–∏–±–∫–æ–π
        """
        # –û—á–∏—â–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º (–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ content)
        cleaned_prompt = prompt.strip() if prompt else ""
        cleaned_system_prompt = system_prompt.strip() if system_prompt else None
        
        if not cleaned_prompt:
            raise ValueError("Prompt cannot be empty")
        
        messages = []
        if cleaned_system_prompt:
            messages.append({"role": "system", "content": cleaned_system_prompt})
        messages.append({"role": "user", "content": cleaned_prompt})
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        primary_model = model or self.primary_model
        
        # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–∑–≤–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        logger.info(f"üöÄ [LLM GENERATE] –ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞")
        logger.info(f"üöÄ [LLM GENERATE] –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.primary_provider}, –º–æ–¥–µ–ª—å: {primary_model}")
        logger.info(f"üöÄ [LLM GENERATE] –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: temperature={temperature}, max_tokens={max_tokens}")
        logger.info(f"üöÄ [LLM GENERATE] –ü—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤): {cleaned_prompt[:300]}...")
        if cleaned_system_prompt:
            logger.info(f"üöÄ [LLM GENERATE] System –ø—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤): {cleaned_system_prompt[:300]}...")
        
        response = await self._call_api(
            provider=self.primary_provider,
            model=primary_model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
        if response.error is None and response.confidence >= self.confidence_threshold:
            logger.info(f"‚úÖ [LLM GENERATE] –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response.confidence:.2f})")
            logger.info(f"‚úÖ [LLM GENERATE] –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {response.content[:500]}...")
            return response
        
        # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω fallback - –ø—Ä–æ–±—É–µ–º —Ü–µ–ø–æ—á–∫—É fallback –º–æ–¥–µ–ª–µ–π
        if use_fallback and self.fallback_chain:
            logger.warning(
                f"‚ö†Ô∏è [LLM GENERATE] –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å {primary_model} –Ω–µ –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É "
                f"(–æ—à–∏–±–∫–∞: {response.error}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response.confidence:.2f}). "
                f"–ü—Ä–æ–±—É—é fallback —Ü–µ–ø–æ—á–∫—É –∏–∑ {len(self.fallback_chain)} –º–æ–¥–µ–ª–µ–π..."
            )
            
            # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –≤ —Ü–µ–ø–æ—á–∫–µ fallback
            for idx, fallback_config in enumerate(self.fallback_chain, 1):
                fallback_provider = fallback_config.get("provider", "openrouter")
                fallback_model = fallback_config.get("model")
                
                if not fallback_model:
                    logger.warning(f"Fallback {idx} skipped: model not specified")
                    continue
                
                logger.info(f"üîÑ [LLM GENERATE] –ü—Ä–æ–±—É—é fallback {idx}/{len(self.fallback_chain)}: {fallback_provider}/{fallback_model}")
                
                fallback_response = await self._call_api(
                    provider=fallback_provider,
                    model=fallback_model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç
                if fallback_response.error is None:
                    logger.info(f"‚úÖ [LLM GENERATE] Fallback {idx} ({fallback_provider}/{fallback_model}) —É—Å–ø–µ—à–Ω–æ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç")
                    logger.info(f"‚úÖ [LLM GENERATE] –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {fallback_response.content[:500]}...")
                    return fallback_response
                else:
                    logger.warning(
                        f"‚ùå [LLM GENERATE] Fallback {idx} ({fallback_provider}/{fallback_model}) –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: "
                        f"{fallback_response.error}"
                    )
            
            logger.error("‚ùå [LLM GENERATE] –í—Å–µ –º–æ–¥–µ–ª–∏ –≤ fallback —Ü–µ–ø–æ—á–∫–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏")
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç (–∏–ª–∏ primary)
        logger.error(f"‚ùå [LLM GENERATE] –í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –∏ fallback –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏. –í–æ–∑–≤—Ä–∞—â–∞—é –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç (–æ—à–∏–±–∫–∞: {response.error})")
        return response
    
    def _get_default_model(self, provider: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (deprecated, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è primary_model)"""
        defaults = {
            "openrouter": "deepseek/deepseek-chat"
        }
        return defaults.get(provider, self.primary_model)
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç"""
        if self._client is not None and not self._client.is_closed:
            await self._client.aclose()
            self._client = None

